{"tag":"AI","isDark":false,"type":"tag","path":"tags/AI/","json_base":"data/tags/AI/","json":"data/tags/AI/1.json","current":1,"total":1,"posts":[{"type":"post","json_base":"data/posts","json":"data/posts/2014/03/20/beat-flappy-2048-with-q-learning.json","path":"2014/03/20/beat-flappy-2048-with-q-learning/","data":{"title":"Beat flappy 2048 with Q Learning","content":"<h2 id=\"u4ECB_u7ECD\"><a href=\"#u4ECB_u7ECD\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>2048这游戏已经被玩坏了，有人把它和Flappy Bird杂交，玩不过不能忍，于是写了个AI玩之。</p>\n<h2 id=\"u6E38_u620F_u6E90_u7801_u4FEE_u6539\"><a href=\"#u6E38_u620F_u6E90_u7801_u4FEE_u6539\" class=\"headerlink\" title=\"游戏源码修改\"></a>游戏源码修改</h2><p>首先需要对游戏进行适当的修改，便于AI获取游戏状态，并输出控制量。</p>\n<p>修改<code>application.js</code>，将几个关键的对象放到<code>windows</code>命名空间中便于访问：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">window</span>.requestAnimationFrame(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">window</span>.input = KeyboardInputManager;</span><br><span class=\"line\">\t<span class=\"built_in\">window</span>.actuator = HTMLActuator;</span><br><span class=\"line\">  <span class=\"built_in\">window</span>.score = LocalScoreManager;</span><br><span class=\"line\">  <span class=\"built_in\">window</span>.game = <span class=\"keyword\">new</span> GameManager(<span class=\"number\">4</span>, KeyboardInputManager, HTMLActuator, LocalScoreManager);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>游戏的逻辑主要在<code>game_manager.js</code>中实现：</p>\n<p>游戏中的“鸟”的css class是<code>tile-bird</code>，障碍物的css class是<code>tile-block</code>，在本文中分别简称为<code>bird</code>和<code>block</code>。</p>\n<ul>\n<li>使用<code>game.jump()</code>跳跃</li>\n<li>bird的状态：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>变量名</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>game.birdpos</code></td>\n<td>顶端的y坐标，$[0, 1]$之间，0为顶端</td>\n</tr>\n<tr>\n<td><code>game.birdspd</code></td>\n<td>y方向速度，向下为正</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>在任意时刻，只有4个<code>block</code>分别称为<code>a</code>, <code>b</code>, <code>c</code>和<code>d</code>，<code>a</code>与<code>b</code>,<code>c</code>与<code>d</code>成组，有相同的水平坐标，两组<code>block</code>之间一直保持2个tile的距离。每组block只有3种可能状态：全在上、全在下以及一上一下，因此block的状态由两个0~2之间的数字<code>game.ab</code>, <code>game.cd</code>确定。</li>\n</ul>\n<p>游戏由一个timer驱动，每一帧计算游戏状态的变化，最后调用<code>window.actuator.actuate()</code>方法计算元素位置，重绘游戏。</p>\n<p>在游戏计算出元素位置并重绘后获取状态，并由AI注入控制量是最为便捷的方式。</p>\n<p>修改<code>html_actuator.js</code>:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HTMLActuator.prototype.actuate = <span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">grid, metadata</span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">//.. Other stuff</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Call AI</span></span><br><span class=\"line\">  <span class=\"built_in\">window</span>.AI.play(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这样对原游戏的改动就完成了，接下来只需要实现AI类，并把类对象赋值到<code>window.AI</code>即可。</p>\n<h2 id=\"Q-Learning\"><a href=\"#Q-Learning\" class=\"headerlink\" title=\"Q-Learning\"></a>Q-Learning</h2><p>Q-Learning是一种强化学习算法，能用于寻找Markov决策过程(MDP, Markov decision process)的最优解。<br>MDP问题模型由一个agent，状态$S$以及每个状态对应动作(action)集合$A$构成。Agent通过完成一个动作，从一个状态$S$跳转到另一个状态$S’$，获得一定的奖励。Agent的目标就是使奖励最大化，通过学习在每个状态下最优的动作，达到这个目的。</p>\n<p>算法的核心是矩阵$Q$，记录状态-动作对的奖励：</p>\n<p>$$Q: S \\times A \\rightarrow \\mathbb{R}$$</p>\n<p>算法初始时，$Q$取设计好的值，随后agent每执行一次动作，观察新状态以及获得的奖励，通过如下公式迭代更新：</p>\n<span>$$Q_{t+1}(s_t, a_t) = Q_{t}(s_t, a_t) + \\alpha_{t}(s_t, a_t) \\times [ R_{t+1} + \\gamma \\max Q_{t}(s_{t+1}, a) - Q_{t}(s_t, a_t) ]$$</span><!-- Has MathJax -->\n<p>其中：</p>\n<ul>\n<li>$Q_{t+1}(s_t, a_t)$: 新的$Q$值</li>\n<li>$Q_{t}(s_t, a_t)$: 上一时刻$Q$值</li>\n<li>$R_{t+1}$: 在$s_t$时执行$a_t$后获得的奖励</li>\n<li>$\\alpha \\in [0, 1]$: learning rate</li>\n<li>$\\gamma$: 折扣率</li>\n</ul>\n<h2 id=\"u7B97_u6CD5_u8BBE_u8BA1\"><a href=\"#u7B97_u6CD5_u8BBE_u8BA1\" class=\"headerlink\" title=\"算法设计\"></a>算法设计</h2><ul>\n<li><p>状态：</p>\n<ul>\n<li>$\\Delta y$: <code>bird</code>到能安全通过当前<code>block</code>最高点的垂直距离</li>\n<li>$\\Delta x$: <code>bird</code>到下一个block的水平方向距离</li>\n</ul>\n</li>\n<li><p>动作：</p>\n<ul>\n<li><code>jump</code>: 跳跃</li>\n<li><code>idle</code>: 不动作</li>\n</ul>\n</li>\n<li><p>奖励：</p>\n<ul>\n<li>死亡：<code>-100</code></li>\n<li>存活：<code>1</code></li>\n</ul>\n</li>\n<li><p>$Q$的初始化</p>\n</li>\n</ul>\n<p>虽然可以简单的把$Q$全初始化为0，但这样会延长学习时间。并且在很多情况下，会导致<code>bird</code>一直跳跃直到跳出顶端掉不下来，这样不管是<code>jump</code>还是<code>idle</code>都会被惩罚，这样永远无法学习到正确行为。另外在底部也会有同样的问题。</p>\n<p>实际实现时，加入了先验知识：</p>\n<ul>\n<li>对所有$\\Delta y &lt; y_{min}$的$s$，初始化<code>jump</code>的reward为<code>-100</code>。即在上端时禁止跳跃</li>\n<li><p>对所有$\\Delta y &gt; n * BirdHeight$的$s$，初始化<code>idle</code>的reward为<code>-5</code>，<code>n</code>接近<code>1</code>。即离最高点的距离小于<code>bird</code>自己高度的时候，倾向于跳跃。注意这里的reward值较小，是因为在某些组合下（如当前<code>block</code>在下，下一个<code>block</code>在上），跳跃会挂掉，值如果过大，$Q$值无法及时对惩罚做出反馈。</p>\n</li>\n<li><p>不定状态时的随机参数</p>\n</li>\n</ul>\n<p>在<code>jump</code>和<code>action</code>的reward相等时，无法通过$Q$做出决策，这个时候需要随机决定采取何种行为。</p>\n<p>实际实现时，同样没有简单的将这个概率设为<code>0.5</code>，而是让不跳跃的概率远大于跳跃。道理很简单，游戏的操作方式是不平衡的，玩家只能干预下落，而不能干预上升，掉得太低跳一下就行了，跳得太高就只有等死。</p>\n<h2 id=\"u6548_u679C\"><a href=\"#u6548_u679C\" class=\"headerlink\" title=\"效果\"></a>效果</h2><p>目前实现的版本在未学习的情况下，可以一次跳跃到700+分，学习一小时后可以到1000分，到后面出错都是遇到比较极端的组合差之毫厘，重现概率不高，所以学习速度会变慢。</p>\n<p>玩：<a href=\"http://catx.me/Q-Learning-Flappy-2048/\">Q Learning Flappy 2048</a></p>\n<p>代码：<a href=\"https://github.com/akfish/Q-Learning-Flappy-2048\" target=\"_blank\" rel=\"external\">GitHub</a></p>\n","date":"2014-03-20T10:53:10.000Z","path":"2014/03/20/beat-flappy-2048-with-q-learning/","isDark":false,"featureColor":"#e3c772","featureImage":"/images/43e55b43715494ac817795932fe4ab2c5b34268a.png","excerpt":"","featureSwatch":{"Vibrant":{"color":"#e3c772","isDark":false,"contrast":1.6563435380830245},"Muted":{"color":"#bcaca3","isDark":false,"contrast":2.194781298784433},"DarkVibrant":{"color":"#6e5815","isDark":true,"contrast":10.222616092326977},"LightVibrant":{"color":"#fcfcec","isDark":false,"contrast":1.0357850628347347},"LightMuted":{"color":"#cbc2b3","isDark":false,"contrast":1.7636756459902587}},"permalink":"http://catx.me/2014/03/20/beat-flappy-2048-with-q-learning/","json":"data/posts/2014/03/20/beat-flappy-2048-with-q-learning.json","tags":[{"name":"AI","slug":"AI","path":"tags/AI/","permalink":"http://catx.me/tags/AI/","postCount":1},{"name":"CoffeeScript","slug":"CoffeeScript","path":"tags/CoffeeScript/","permalink":"http://catx.me/tags/CoffeeScript/","postCount":6},{"name":"Fun","slug":"Fun","path":"tags/Fun/","permalink":"http://catx.me/tags/Fun/","postCount":2},{"name":"JavaScript","slug":"JavaScript","path":"tags/JavaScript/","permalink":"http://catx.me/tags/JavaScript/","postCount":1}],"categories":[{"name":"挨踢","slug":"挨踢","path":"categories/挨踢/","permalink":"http://catx.me/categories/挨踢/","postCount":17}]},"sha1":"ea834ca2055280b5fdf2042dbebcf028508b19d8","isDigest":true}],"sha1":"8c97e3ef3cb1bcfb2808dfc94fe0bbd12359a39a"}